{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Initial Parsing\n",
    "\n",
    "Before doing more fine-grained parsing on individual resume sections (Education, Experience, etc.), we need to parse the sections out\n",
    "\n",
    "Couple of options:\n",
    "\n",
    "1. Regex-based\n",
    "2. Text-only LLM\n",
    "3. Vision LLM (for PDFs that are not formatted with text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "from pydantic import BaseModel, RootModel, Field\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "from pytesseract import pytesseract\n",
    "\n",
    "from utils.with_structured_output import with_structured_output\n",
    "from utils.extract_pdf_text import extract_pdf_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Key Information With Regex\n",
    "\n",
    "1. Phone number\n",
    "2. Email\n",
    "3. LinkedIn\n",
    "4. GitHub\n",
    "5. Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phone number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_phone_number(text: str) -> str:\n",
    "    phone_pattern = re.compile(r\"\"\"\n",
    "        (?:\\+(?P<country_code>1)+\\s)?   # Match country code (if exists)\n",
    "        (?:\\()?\n",
    "        (?P<area_code>\\d{3})            # Match area code (first 3 digits)\n",
    "        (?:[.-]|\\)\\s?)\n",
    "        (?P<prefix>\\d{3})               # Match prefix (second 3 digits)\n",
    "        [.-]\n",
    "        (?P<line_number>\\d{4})          # Match line number (last 4 digits)\n",
    "    \"\"\", re.VERBOSE)\n",
    "    match = re.search(phone_pattern, text)\n",
    "    if match:\n",
    "        return f\"{match.group(\"area_code\")}-{match.group(\"prefix\")}-{match.group(\"line_number\")}\"\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'832-416-3570'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_number = \"(832) 416-3570\"\n",
    "\n",
    "parse_phone_number(phone_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_email(text: str) -> str:\n",
    "    email_pattern = re.compile(r\"\"\"\n",
    "        \\b\n",
    "        [A-Za-z0-9._%+-]+   # Local part\n",
    "        @\n",
    "        [A-Za-z0-9.-]+      # Domain \n",
    "        \\.[A-Za-z]{2,}\n",
    "        \\b\n",
    "    \"\"\", re.VERBOSE)\n",
    "    match = re.search(email_pattern, text)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kevzhang2022@tamu.edu'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email = \"Some leading text: kevzhang2022@tamu.edu and trailing text\"\n",
    "\n",
    "parse_email(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinkedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_linkedin(text: str) -> str:\n",
    "    linkedin_pattern = r\"(?:(?:https://)?(?:www.)?linkedin.com/in/(?P<profile_id>[A-Za-z0-9-]{5,30})/?\\b)\"\n",
    "    match = re.search(linkedin_pattern, text)\n",
    "    if match:\n",
    "        return f\"https://linkedin.com/in/{match.group(\"profile_id\")}\"\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://linkedin.com/in/kevinkz'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linkedin = \"LinkedIn: linkedin.com/in/kevinkz some trailing text\"\n",
    "\n",
    "parse_linkedin(linkedin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_github(text: str) -> str:\n",
    "    github_pattern = r\"(?:(?:https://)?(?:www.)?github.com/(?P<username>[A-Za-z0-9-]{1,39})/?\\b)\"\n",
    "    match = re.search(github_pattern, text)\n",
    "    if match:\n",
    "        return f\"https://github.com/{match.group(\"username\")}\"\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://github.com/n1v3x2'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "github = \"GitHub: github.com/n1v3x2 some trailing text\"\n",
    "\n",
    "parse_github(github)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_location(text: str) -> str:\n",
    "    location_pattern = r\"(?P<city>\\b[A-Za-z0-9]+(?:(?:[ .'-]|. )[A-Za-z0-9]+)*),\\s?(?P<state>[A-Z]{2})\"\n",
    "    match = re.search(location_pattern, text)\n",
    "    if match:\n",
    "        return f\"{match.group(\"city\")}, {match.group(\"state\")}\"\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'College Station, TX'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location = \"College Station, TX\"\n",
    "\n",
    "parse_location(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tying it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_text = extract_pdf_text(\"../sample_data/Kareem_resume.pdf\")\n",
    "parsed_info = {\n",
    "    \"Phone\": parse_phone_number(resume_text),\n",
    "    \"Email\": parse_email(resume_text),\n",
    "    \"LinkedIn\": parse_linkedin(resume_text),\n",
    "    \"GitHub\": parse_github(resume_text),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Phone': '979-820-2212',\n",
       " 'Email': 'knkabbani@gmail.com',\n",
       " 'LinkedIn': 'https://linkedin.com/in/kareem-kabbani',\n",
       " 'GitHub': 'https://github.com/kkabbani05'}"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Purely Regex-Based Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading_map = {\n",
    "    # Experience\n",
    "    r\"(Work|Relevant|Professional)?\\s*(Experience|History)\": \"Experience\",\n",
    "    r\"(Employment|Career)\\s*(History|Experience)\": \"Experience\",\n",
    "    r\"(Internship|Internships|Intern Experiences?)\": \"Experience\",\n",
    "    r\"(Freelance|Contract)\\s*(Work|Experience)\": \"Experience\",\n",
    "    r\"Work\": \"Experience\",\n",
    "\n",
    "    # Education\n",
    "    r\"(Education|Educational Background|Academic History|Academic Background)\": \"Education\",\n",
    "    r\"(Certifications|Courses|Licenses|Trainings|Accreditations)\": \"Certifications\",\n",
    "    r\"(Professional Development|Learning)\": \"Certifications\",\n",
    "\n",
    "    # Skills\n",
    "    r\"(Skills|Technical Skills|Key Competencies|Core Competencies|Abilities)\": \"Skills\",\n",
    "    r\"(Technical Proficiencies|Technical Expertise|Expertise|Proficiencies)\": \"Skills\",\n",
    "    r\"(Languages|Programming Languages)\": \"Skills\",\n",
    "    \n",
    "    # Projects\n",
    "    r\"(Projects|Key Projects|Personal Projects|Side Projects)\": \"Projects\",\n",
    "    r\"(Freelance Projects|Independent Projects|Portfolio)\": \"Projects\",\n",
    "\n",
    "    # Achievements and Awards\n",
    "    r\"(Achievements?|Awards?|Honors?|Accolades?|Recognitions?)\": \"Achievements\",\n",
    "    r\"(Accomplishments|Milestones)\": \"Achievements\",\n",
    "\n",
    "    # Volunteer Work\n",
    "    r\"(Volunteer|Volunteering|Community( Service)?|Volunteer Experience)\": \"Volunteer Work\",\n",
    "    r\"(Social Work|Non-Profit Work)\": \"Volunteer Work\",\n",
    "\n",
    "    # Leadership\n",
    "    r\"(Leadership|Leadership Experience|Leadership Roles|Positions of Responsibility)\": \"Leadership\",\n",
    "    r\"(Managerial Experience|Team Leadership|Organizational Roles)\": \"Leadership\",\n",
    "\n",
    "    # Publications and Research\n",
    "    r\"(Publications?|Research|Academic Papers|Articles|Journals?)\": \"Publications\",\n",
    "    r\"(Research Projects|Thesis|Dissertation)\": \"Research\",\n",
    "\n",
    "    # Interests and Hobbies\n",
    "    r\"(Interests?|Hobbies?|(Extracurricular|Collegiate) Activities)\": \"Interests\",\n",
    "    r\"(Passions?|Leisure Activities)\": \"Interests\",\n",
    "\n",
    "    # Objective or Summary\n",
    "    r\"(Objective|Career Objective|Professional Objective)\": \"Summary\",\n",
    "    r\"(Summary|Professional (Highlights|Profile|Summary)|Career Summary)\": \"Summary\",\n",
    "\n",
    "    # References\n",
    "    r\"(References?|Professional References|Referees?)\": \"References\",\n",
    "}\n",
    "\n",
    "def normalize_heading(heading):\n",
    "    for pattern, normalized_heading in heading_map.items():\n",
    "        if re.search(pattern, heading, re.IGNORECASE):\n",
    "            return normalized_heading\n",
    "    return \"Miscellaneous\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sections_by_heading(resume_text: str) -> dict[str, str]:\n",
    "    heading_pattern = r\"\"\"\n",
    "        ^(                                  \n",
    "            [A-Z][a-z]+(?:\\ [A-Z][a-z]+)?(?:[\\s]*\\n)    # Matches Captialized headings\n",
    "            |\n",
    "            [A-Z]{3,}(?:\\ [A-Z]{2,})*(?::?[\\s]*\\n)      # Matches ALL CAPS headings \n",
    "        )\n",
    "    \"\"\"\n",
    "    heading_regex = re.compile(heading_pattern, re.VERBOSE | re.MULTILINE)\n",
    "    matches = list(re.finditer(heading_regex, resume_text))\n",
    "    \n",
    "    sections = defaultdict(list)\n",
    "    for i, match in enumerate(matches):\n",
    "        # The section starts at the end of the heading\n",
    "        start = match.end()\n",
    "        # The section ends at the start of the next heading or the end of the resume\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(resume_text)\n",
    "        \n",
    "        heading = match.group(1).strip()\n",
    "        normalized_heading = normalize_heading(heading)\n",
    "        \n",
    "        if normalized_heading == \"Miscellaneous\":\n",
    "            sections[normalized_heading].append(resume_text[start:end].strip())\n",
    "        else :\n",
    "            sections[normalized_heading] = resume_text[start:end].strip()\n",
    "    \n",
    "    return dict(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Education': 'Texas A&M University, College Station, Texas December 2025\\n'\n",
      "              'Bachelor of Science in Aerospace Engineering, Minor in '\n",
      "              'Mathematics\\n'\n",
      "              'GPA: 4.0',\n",
      " 'Experience': 'Sandia National Laboratories, Albuquerque, New Mexico\\n'\n",
      "               'Applied Aerodynamics R&D Intern Aug 2024 – Present\\n'\n",
      "               'Nuclear Deterrence R&D Intern May 2024 – Aug 2024\\n'\n",
      "               'NASA Armstrong Flight Research Center, Edwards, California Jan '\n",
      "               '2024 – May 2024\\n'\n",
      "               'Aerostructures Research Engineering Intern\\n'\n",
      "               '• Conducted disk (bubble) testing of nylon fabric to validate '\n",
      "               'strain sensors for use on parachute canopies.\\n'\n",
      "               '• Directed 12 disk tests to failure to investigate fabric '\n",
      "               'failure modes and maximum operating pressure.\\n'\n",
      "               '• Programmed MATLAB script to measure strains up to 20% from '\n",
      "               'side-profile images of disk tests.\\n'\n",
      "               '• Directed 10 tests to validate side-profile script against '\n",
      "               'strain measured by Digital Image Correlation (DIC).\\n'\n",
      "               '• Analyzed DIC data using GOM software, yielding script '\n",
      "               'relative error below 5% on average.\\n'\n",
      "               '• Directed 10 disk tests of nylon embedded with a 2-inch '\n",
      "               'commercial off-the-shelf capacitive strain sensor.\\n'\n",
      "               '• Used side-profile script to measure strain under the '\n",
      "               'capacitive sensor as a source of validation data.\\n'\n",
      "               '• Laser-cut over 100 nylon and silicon coupons for disk '\n",
      "               'testing of parachute broadcloth.\\n'\n",
      "               '• Added configuration-saving functionality to Python GUI '\n",
      "               'developed for a NASA-made portable DAQ.\\n'\n",
      "               'Klebanoff-Saric Wind Tunnel, College Station, Texas Jun 2023 – '\n",
      "               'Dec 2023\\n'\n",
      "               'Undergraduate Research Assistant\\n'\n",
      "               '• Developed LabVIEW code to operate tunnel fan and National '\n",
      "               'Instruments DAQ during daily calibration.\\n'\n",
      "               '• Created Python scripts to compute 7 nonlinear regression '\n",
      "               'parameters relating wind speed to Constant\\n'\n",
      "               'Temperature Anemometer voltage and tunnel temperature, and '\n",
      "               'integrated these scripts into LabVIEW code.\\n'\n",
      "               '• Programmed Monte Carlo simulation (500+ realizations) to '\n",
      "               'estimate parameter uncertainty from sensor error.\\n'\n",
      "               '• Wrote 10-page report detailing integration of Python and '\n",
      "               'LabVIEW scripts.\\n'\n",
      "               '• Increased data visualization efficiency by writing Python '\n",
      "               'script to load and format data in Tecplot360.\\n'\n",
      "               '• Operated a low-speed (2-15 m/s) wind tunnel in support of '\n",
      "               'boundary layer experiments involving Constant\\n'\n",
      "               'Temperature Anemometry and naphthalene flow visualization.\\n'\n",
      "               'Technical Projects and Research\\n'\n",
      "               'Texas A&M AIAA CanSat Team, College Station, Texas Oct 2022 – '\n",
      "               'May 2024\\n'\n",
      "               'Electrical Team Lead\\n'\n",
      "               '• Designed electrical power and avionics subsystems of a 300 '\n",
      "               'cubic inch satellite for AIAA’s annual CanSat\\n'\n",
      "               'design-build-launch competition with a team of two other '\n",
      "               'students.\\n'\n",
      "               '• Created schedules for electrical system progress and '\n",
      "               'assigned tasks to team members.\\n'\n",
      "               '• Utilized EasyEDA to create preliminary PCB design, fitting '\n",
      "               'over 15 components into a 63 mm radius space.\\n'\n",
      "               '• Performed trade studies of air pressure sensors, pitot '\n",
      "               'tubes, microcontrollers, GPS units, XBee radios,\\n'\n",
      "               'gyroscopes, and cameras with a $1000 budget and 700g mass '\n",
      "               'budget.\\n'\n",
      "               '• Presented 10 slides covering the electrical power subsystem, '\n",
      "               'microcontroller, and inertial measurement unit\\n'\n",
      "               'to competition judges as part of the Preliminary Design Review '\n",
      "               'and Cumulative Design Review.\\n'\n",
      "               '• Soldered over 20 electrical components and 2 independent '\n",
      "               'electrical systems on satellite, including avionics\\n'\n",
      "               'and power for each system.\\n'\n",
      "               'Texas A&M Turbomachinery Lab, College Station, Texas Feb 2023 '\n",
      "               '– Dec 2023',\n",
      " 'Miscellaneous': ['www.linkedin.com/in/bmayeux03 ♦ bmayeux@tamu.edu'],\n",
      " 'Publications': '• Conducted research aiming to predict hazards associated '\n",
      "                 'with Li-Ion battery combustion, including amount of\\n'\n",
      "                 'energy released, final temperature, and chemical composition '\n",
      "                 'of reaction products.\\n'\n",
      "                 '• Reviewed scientific literature to determine thermal '\n",
      "                 'decomposition properties of various battery cathodes.'}\n"
     ]
    }
   ],
   "source": [
    "resume_text = extract_pdf_text(\"../sample-data/Ben_Resume.pdf\")\n",
    "resume_sections = extract_sections_by_heading(resume_text)\n",
    "pprint(resume_sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex-based parsing is too inflexible for handling a wide variety of resume formats... maybe an LLM-based solution would work better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: LLM with Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resume(BaseModel):\n",
    "    experience: str = Field(..., alias=\"Experience\")\n",
    "    education: str  = Field(..., alias=\"Education\")\n",
    "    skills: str     = Field(..., alias=\"Skills\")\n",
    "    projects: str   = Field(..., alias=\"Projects\")\n",
    "    leadership: str = Field(..., alias=\"Leadership\")\n",
    "    research: str   = Field(..., alias=\"Research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_EXTRACTION_PROMPT = \"\"\"\n",
    "You are an expert at parsing resume information. Given resume text, your job is to parse individual sections based on resume heading. Follow this format:\n",
    "    {{\n",
    "        \"Experience\": \"<Experience>\",\n",
    "        \"Education\": \"<Education>\",\n",
    "        \"Skills\": \"<Skills>\",\n",
    "        \"Projects\": \"<Projects>\",\n",
    "        \"Leadership\": \"<Leadership Experience>\",\n",
    "        \"Research\": \"<Research Experience>\"\n",
    "    }}\n",
    "    \n",
    "Your job is very simple: simply copy everything under each resume section into the output format; do not worry about formatting. \n",
    "\n",
    "If a resume does not contain one of the sections, output an empty string for that section. For example, if there is no \"Leadership\" section in the resume, the output will be `\"Leadership\": \"\"`.\n",
    "\n",
    "In the experience section, **ensure that you include company names**, which are usually listed beside or under position names.\n",
    "\n",
    "**The parsed information must be explicitly contained in the resume.**\n",
    "\n",
    "**Do not exclude any information from the resume.**\n",
    "\n",
    "Resume:\n",
    "{resume_text}\n",
    "\n",
    "Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_text = extract_pdf_text(\"../input/resumes/Kevin_resume.pdf\")\n",
    "parsed_resume = with_structured_output(\n",
    "    prompt=INITIAL_EXTRACTION_PROMPT.format(resume_text=resume_text),\n",
    "    schema=Resume,\n",
    "    model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../output/parsed_resume.json\", \"w\") as file:\n",
    "    json.dump(parsed_resume, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nFinal Answer: The final answer is 100. I hope it is correct. \\n\\nPlease let me know if I need to make any changes. \\n\\nI will make sure to follow the format to the letter as requested. \\n\\nThe final answer is'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_API_KEY = os.getenv(\"HF_API_KEY\")\n",
    "client = InferenceClient(token=HF_API_KEY)\n",
    "\n",
    "response = client.text_generation(\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    prompt=\"Answer concisely: What is 10 x 10?\",\n",
    "    max_new_tokens=50)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "(Request ID: 6pyDhKkwQmqlIJUXhutQk)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/resume/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/resume/lib/python3.12/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.1-8B-Instruct",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-3.1-8B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINITIAL_EXTRACTION_PROMPT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_text\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mResume\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_json_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/resume/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:2332\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[0;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[1;32m   2307\u001b[0m         _set_unsupported_text_generation_kwargs(model, unused_params)\n\u001b[1;32m   2308\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_generation(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2309\u001b[0m             prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m   2310\u001b[0m             details\u001b[38;5;241m=\u001b[39mdetails,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2330\u001b[0m             watermark\u001b[38;5;241m=\u001b[39mwatermark,\n\u001b[1;32m   2331\u001b[0m         )\n\u001b[0;32m-> 2332\u001b[0m     \u001b[43mraise_text_generation_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[38;5;66;03m# Parse output\u001b[39;00m\n\u001b[1;32m   2335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/resume/lib/python3.12/site-packages/huggingface_hub/inference/_common.py:466\u001b[0m, in \u001b[0;36mraise_text_generation_error\u001b[0;34m(http_error)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhttp_error\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# Otherwise, fallback to default error\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m http_error\n",
      "File \u001b[0;32m/opt/anaconda3/envs/resume/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:2302\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[0;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[1;32m   2300\u001b[0m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[1;32m   2301\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2302\u001b[0m     bytes_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2303\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2304\u001b[0m     match \u001b[38;5;241m=\u001b[39m MODEL_KWARGS_NOT_USED_REGEX\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/resume/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:296\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 296\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/resume/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:460\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[1;32m    457\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m endpoint:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m endpoint_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m     )\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(BadRequestError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m    463\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure your token has the correct permissions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    467\u001b[0m     )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: (Request ID: 6pyDhKkwQmqlIJUXhutQk)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query."
     ]
    }
   ],
   "source": [
    "response = client.text_generation(\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    prompt=INITIAL_EXTRACTION_PROMPT.format(resume_text=resume_text),\n",
    "    grammar={\n",
    "        \"type\": \"json\",\n",
    "        \"value\": Resume.model_json_schema()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need HF Pro subscription to access some Llama models; I think Ollama might be my best option after all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3: Vision Model Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_pdf_text(\"../sample_data/Kevin_resume_img.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the PDF to an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 1\n"
     ]
    }
   ],
   "source": [
    "images = convert_from_path(\"../sample_data/Ben_resume.pdf\", dpi=300)\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    image.save(f\"output/Ben_resume.jpg\", \"JPEG\")\n",
    "    print(f\"Saved page {i + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the vision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_EXTRACTION_PROMPT = \"\"\"\n",
    "You are an expert at parsing resume information from an image. Given an image of a resume, your job is to parse individual sections based on resume heading. Follow this format:\n",
    "    {{\n",
    "        \"Experience\": \"<Experience>\",\n",
    "        \"Education\": \"<Education>\",\n",
    "        \"Skills\": \"<Skills>\",\n",
    "        \"Projects\": \"<Projects>\",\n",
    "        \"Leadership\": \"<Leadership Experience>\",\n",
    "        \"Research\": \"<Research Experience>\"\n",
    "    }}\n",
    "    \n",
    "Your job is very simple: simply copy everything under each resume section into the output format; do not worry about formatting. \n",
    "\n",
    "If a resume does not contain one of the sections, output an empty string for that section. For example, if there is no \"Leadership\" section in the resume, the output will be `\"Leadership\": \"\"`.\n",
    "\n",
    "In the experience section, **ensure that you include company names**, which are usually listed beside or under position names.\n",
    "\n",
    "**The parsed information must be explicitly contained in the resume.**\n",
    "\n",
    "**Do not exclude any information from the resume.**\n",
    "\n",
    "Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "\n",
    "response = chat(\n",
    "    model=\"llama3.2-vision\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": IMAGE_EXTRACTION_PROMPT,\n",
    "            \"images\": [\"output/Ben_resume.jpg\"]\n",
    "        }\n",
    "    ],\n",
    "    format=Resume.model_json_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Education': 'Texas A&M University, College Station, Texas: Bachelor of '\n",
      "              'Science in Aerospace Engineering, Minor in Mathematics, GPA 4.0',\n",
      " 'Experience': 'Sandia National Laboratories, Albuquerque, New Mexico: Applied '\n",
      "               'Aerodynamics R&D Intern; NASA Armstrong Flight Research '\n",
      "               'Center, Edwards, California: Aerostructures Research '\n",
      "               'Engineering Intern; Klebanoff-Saric Wind Tunnel, College '\n",
      "               'Station, Texas: Undergraduate Research Assistant',\n",
      " 'Leadership': '',\n",
      " 'Projects': 'Klebanoff-Saric Wind Tunnel, College Station, Texas: Developed '\n",
      "             'LabVIEW code to operate tunnel fan and National Instruments DAQ '\n",
      "             'during daily calibration.',\n",
      " 'Research': 'Texas A&M University, College Station, Texas: Undergraduate '\n",
      "             'Researcher',\n",
      " 'Skills': ''}\n"
     ]
    }
   ],
   "source": [
    "parsed_resume = json.loads(response.message.content)\n",
    "pprint(parsed_resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bad with VLMs:\n",
    "1. Most state-of-the-art VLMs cannot process very large images (e.g., Llama3.2-vision can only process images up to 1120x1120)\n",
    "2. llama3.2-vision runs pretty slow on my computer, probably because it requires 11B parameters vs only 8B from llama3.1\n",
    "3. llama3.2-vision isn't very good at parsing a lot of textual information from resumes; it's missing a lot of information from the \"Experience\" section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Option 4: OCR -> LLM parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_images = convert_from_path(\"../sample_data/Kevin_resume.pdf\", dpi=300)\n",
    "pdf_images[0].save(\"output/Kevin_resume.png\", \"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_pdf_text = pytesseract.image_to_string(pdf_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Kevin Zhang\\n'\n",
      " '\\n'\n",
      " '(832) 416-3570 | kevzhang2022@gmail.com | linkedin.com/in/kevinkz | '\n",
      " 'github.com/nlv3x2\\n'\n",
      " '\\n'\n",
      " 'EDUCATION\\n'\n",
      " 'Texas A&M University May 2026\\n'\n",
      " 'BS in Computer Science, Minor in Statistics and Math College Station, TX\\n'\n",
      " '\\n'\n",
      " 'Cumulative GPA: 4.0/4.0\\n'\n",
      " 'Honors: Dean’s Honor Roll, Engineering Honors (EH), Dean’s Excellence Award '\n",
      " 'Semi-finalist\\n'\n",
      " 'Coursework: Data Structures & Algorithms, Software Engineering, Computer '\n",
      " 'Systems, Discrete Math, Linear Algebra\\n'\n",
      " '\\n'\n",
      " 'EXPERIENCE\\n'\n",
      " 'AI/ML Intern Aug 2024 — Dec 2024\\n'\n",
      " 'Sandia National Laboratories Remote\\n'\n",
      " '\\n'\n",
      " '¢ Developed knowledge graph (KG) generation pipeline with internal LLM '\n",
      " 'microservices to allow multi-hop\\n'\n",
      " 'reasoning in 3-stage retrieval augmented generation (RAG) pipeline\\n'\n",
      " '\\n'\n",
      " '¢ Extracted 30+ domain-specific seed topics from text corpus with BERTopic '\n",
      " 'for KG subgraph creation\\n'\n",
      " '\\n'\n",
      " '¢ Achieved 100% schema-compliant LLM outputs via prompt engineering and '\n",
      " 'grammar-contrained decoding\\n'\n",
      " '\\n'\n",
      " '¢ Packaged KG generation logic into reusable, object-oriented Python modules '\n",
      " 'used by 30 developers\\n'\n",
      " '\\n'\n",
      " 'Software Engineering Intern May 2024 — Aug 2024\\n'\n",
      " 'Sandia National Laboratories Albuquerque, NM\\n'\n",
      " '¢ Redesigned 30 year old Java data analysis suite architecture, cutting '\n",
      " 'developer onboarding time by 3 weeks\\n'\n",
      " '¢ Used MATLAB profiler to find bottleneck in data preprocessing script, '\n",
      " 'leading to 90% execution time reduction\\n'\n",
      " '¢ Built Java class to interface with Javalin REST API, enabling '\n",
      " 'multithreaded network communication\\n'\n",
      " '¢ Reduced logic errors by 50% via integration and regression testing in '\n",
      " 'Jenkins CI/CD pipeline\\n'\n",
      " '\\n'\n",
      " 'Undergraduate Teaching Assistant Jan 2024 — Present\\n'\n",
      " 'Texas A&M University College Station, TX\\n'\n",
      " '\\n'\n",
      " '¢ Leading data structures & algorithms recitations for 60 students '\n",
      " 'semiweekly\\n'\n",
      " '\\n'\n",
      " '¢ Mentoring 15+ students weekly in C++ assignments, leading to 80% reduction '\n",
      " 'in assignment errors\\n'\n",
      " '\\n'\n",
      " 'e Teaching students core CS concepts including vectors, trees, graphs, '\n",
      " 'sorting algorithms, and recursion\\n'\n",
      " 'Full Stack Developer Intern Jun 2022 — Aug 2022, May 2023 — Aug 2023\\n'\n",
      " 'Vertical Automation and Information Technology Katy, TX\\n'\n",
      " '\\n'\n",
      " '¢ Built CRM system with ASP.NET MVC serving 10 users, improving employee '\n",
      " 'efficiency by 50%\\n'\n",
      " '\\n'\n",
      " '¢ Optimized MySQL database performance by eliminating 1,000 duplicate '\n",
      " 'records, improving query speed by 10%\\n'\n",
      " '\\n'\n",
      " 'e Designed scalable SQL database architecture to support complex entity '\n",
      " 'relationships\\n'\n",
      " '\\n'\n",
      " '¢ Created role-based authorization system with Entity Framework to '\n",
      " 'facilitate project management for managers\\n'\n",
      " '\\n'\n",
      " 'PROJECTS\\n'\n",
      " '\\n'\n",
      " 'Credit Card Recommender (Best Financial Hack) | Python, Jupyter, Flask, '\n",
      " 'React, LangChain Nov 2024\\n'\n",
      " '¢ Developed LLM-based credit card recommendation system at 24-hour '\n",
      " 'hackathon, competing against 14 teams\\n'\n",
      " '* Devised chain-of-thought (CoT) recommendation prompt that takes user '\n",
      " 'spending habits into account\\n'\n",
      " 'e« Augmented recommendations with self-curated JSON database containing '\n",
      " 'statistics on 50 popular credit cards\\n'\n",
      " '\\n'\n",
      " 'Point of Sales System | React, TypeScript, Java, Spring Boot, PostgreSQL, '\n",
      " 'JDBC, Git Sep 2024 — Nov 2024\\n'\n",
      " '¢ Designed modular React components and managed application state with React '\n",
      " 'hooks (useState, useContext)\\n'\n",
      " '¢ Orchestrated integration of Google OAuth API to manage '\n",
      " 'authentication/authorization\\n'\n",
      " 'e¢ Implemented JDBC backend to query PostgreSQL database and map results to '\n",
      " 'Java data transfer objects\\n'\n",
      " '¢ Reviewed 60+ GitHub pull requests, leading to 80% reduction in code '\n",
      " 'smells\\n'\n",
      " '\\n'\n",
      " 'TECHNICAL SKILLS\\n'\n",
      " '\\n'\n",
      " 'Languages: Java, Python, C/C++, TypeScript/JavaScript, C#, SQL, HTML/CSS, '\n",
      " 'MATLAB, Bash\\n'\n",
      " 'Frameworks: React, Bootstrap, Flask, JUnit, ASP.NET, Entity Framework, '\n",
      " 'Spring Boot\\n'\n",
      " '\\n'\n",
      " 'Libraries: Numpy, Pandas, Matplotlib, LangChain, OpenAI, Pydantic, '\n",
      " 'TensorFlow\\n'\n",
      " '\\n'\n",
      " 'Developer Tools: Linux, Git (GitHub, Gerrit), Anaconda, Jupyter, Azure '\n",
      " 'DevOps, Jenkins, Vim, Docker\\n'\n",
      " 'Databases: PostgreSQL, MySQL, Neo4J\\n')\n"
     ]
    }
   ],
   "source": [
    "pprint(extracted_pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resume(BaseModel):\n",
    "    experience: str = Field(..., alias=\"Experience\")\n",
    "    education: str  = Field(..., alias=\"Education\")\n",
    "    skills: str     = Field(..., alias=\"Skills\")\n",
    "    projects: str   = Field(..., alias=\"Projects\")\n",
    "    leadership: str = Field(..., alias=\"Leadership\")\n",
    "    research: str   = Field(..., alias=\"Research\")\n",
    "    \n",
    "INITIAL_EXTRACTION_PROMPT = \"\"\"\n",
    "You are an expert at parsing resume information. Given resume text, your job is to parse individual sections based on resume heading. Follow this format:\n",
    "    {{\n",
    "        \"Experience\": \"<Experience>\",\n",
    "        \"Education\": \"<Education>\",\n",
    "        \"Skills\": \"<Skills>\",\n",
    "        \"Projects\": \"<Projects>\",\n",
    "        \"Leadership\": \"<Leadership Experience>\",\n",
    "        \"Research\": \"<Research Experience>\"\n",
    "    }}\n",
    "    \n",
    "Your job is very simple: simply copy everything under each resume section into the output format; do not worry about formatting. \n",
    "\n",
    "If a resume does not contain one of the sections, output an empty string for that section. For example, if there is no \"Leadership\" section in the resume, the output will be `\"Leadership\": \"\"`.\n",
    "\n",
    "In the experience section, **ensure that you include company names**, which are usually listed beside or under position names.\n",
    "\n",
    "**The parsed information must be explicitly contained in the resume.**\n",
    "\n",
    "**Do not exclude any information from the resume.**\n",
    "\n",
    "Resume:\n",
    "{resume_text}\n",
    "\n",
    "Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_resume = with_structured_output(\n",
    "    prompt=INITIAL_EXTRACTION_PROMPT.format(resume_text=extracted_pdf_text),\n",
    "    schema=Resume,\n",
    "    model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Education': 'Texas A&M University May 2026\\n'\n",
      "              'BS in Computer Science, Minor in Statistics and Math College '\n",
      "              'Station, TX\\n'\n",
      "              'Cumulative GPA: 4.0/4.0\\n'\n",
      "              'Honors: Dean’s Honor Roll, Engineering Honors (EH), Dean’s '\n",
      "              'Excellence Award Semi-finalist\\n'\n",
      "              'Coursework: Data Structures & Algorithms, Software Engineering, '\n",
      "              'Computer Systems, Discrete Math, Linear Algebra',\n",
      " 'Experience': 'AI/ML Intern Aug 2024 — Dec 2024\\n'\n",
      "               'Sandia National Laboratories Remote\\n'\n",
      "               '¢ Developed knowledge graph (KG) generation pipeline with '\n",
      "               'internal LLM microservices to allow multi-hop\\n'\n",
      "               'reasoning in 3-stage retrieval augmented generation (RAG) '\n",
      "               'pipeline\\n'\n",
      "               '¢ Extracted 30+ domain-specific seed topics from text corpus '\n",
      "               'with BERTopic for KG subgraph creation\\n'\n",
      "               '¢ Achieved 100% schema-compliant LLM outputs via prompt '\n",
      "               'engineering and grammar-contrained decoding\\n'\n",
      "               '¢ Packaged KG generation logic into reusable, object-oriented '\n",
      "               'Python modules used by 30 developers\\n'\n",
      "               'Software Engineering Intern May 2024 — Aug 2024\\n'\n",
      "               'Sandia National Laboratories Albuquerque, NM\\n'\n",
      "               '¢ Redesigned 30 year old Java data analysis suite '\n",
      "               'architecture, cutting developer onboarding time by 3 weeks\\n'\n",
      "               '¢ Used MATLAB profiler to find bottleneck in data '\n",
      "               'preprocessing script, leading to 90% execution time reduction\\n'\n",
      "               '¢ Built Java class to interface with Javalin REST API, '\n",
      "               'enabling multithreaded network communication\\n'\n",
      "               '¢ Reduced logic errors by 50% via integration and regression '\n",
      "               'testing in Jenkins CI/CD pipeline\\n'\n",
      "               'Undergraduate Teaching Assistant Jan 2024 — Present\\n'\n",
      "               'Texas A&M University College Station, TX\\n'\n",
      "               '¢ Leading data structures & algorithms recitations for 60 '\n",
      "               'students semiweekly\\n'\n",
      "               '¢ Mentoring 15+ students weekly in C++ assignments, leading to '\n",
      "               '80% reduction in assignment errors\\n'\n",
      "               'e Teaching students core CS concepts including vectors, trees, '\n",
      "               'graphs, sorting algorithms, and recursion\\n'\n",
      "               'Full Stack Developer Intern Jun 2022 — Aug 2022, May 2023 — '\n",
      "               'Aug 2023\\n'\n",
      "               'Vertical Automation and Information Technology Katy, TX\\n'\n",
      "               '¢ Built CRM system with ASP.NET MVC serving 10 users, '\n",
      "               'improving employee efficiency by 50%\\n'\n",
      "               '¢ Optimized MySQL database performance by eliminating 1,000 '\n",
      "               'duplicate records, improving query speed by 10%\\n'\n",
      "               'e Designed scalable SQL database architecture to support '\n",
      "               'complex entity relationships\\n'\n",
      "               '¢ Created role-based authorization system with Entity '\n",
      "               'Framework to facilitate project management for managers',\n",
      " 'Leadership': '',\n",
      " 'Projects': 'Credit Card Recommender (Best Financial Hack) | Python, Jupyter, '\n",
      "             'Flask, React, LangChain Nov 2024\\n'\n",
      "             '¢ Developed LLM-based credit card recommendation system at '\n",
      "             '24-hour hackathon, competing against 14 teams\\n'\n",
      "             '* Devised chain-of-thought (CoT) recommendation prompt that '\n",
      "             'takes user spending habits into account\\n'\n",
      "             'e« Augmented recommendations with self-curated JSON database '\n",
      "             'containing statistics on 50 popular credit cards\\n'\n",
      "             'Point of Sales System | React, TypeScript, Java, Spring Boot, '\n",
      "             'PostgreSQL, JDBC, Git Sep 2024 — Nov 2024\\n'\n",
      "             '¢ Designed modular React components and managed application '\n",
      "             'state with React hooks (useState, useContext)\\n'\n",
      "             '¢ Orchestrated integration of Google OAuth API to manage '\n",
      "             'authentication/authorization\\n'\n",
      "             'e¢ Implemented JDBC backend to query PostgreSQL database and map '\n",
      "             'results to Java data transfer objects\\n'\n",
      "             '¢ Reviewed 60+ GitHub pull requests, leading to 80% reduction in '\n",
      "             'code smells',\n",
      " 'Research': '',\n",
      " 'Skills': 'Languages: Java, Python, C/C++, TypeScript/JavaScript, C#, SQL, '\n",
      "           'HTML/CSS, MATLAB, Bash\\n'\n",
      "           'Frameworks: React, Bootstrap, Flask, JUnit, ASP.NET, Entity '\n",
      "           'Framework, Spring Boot\\n'\n",
      "           'Libraries: Numpy, Pandas, Matplotlib, LangChain, OpenAI, Pydantic, '\n",
      "           'TensorFlow\\n'\n",
      "           'Developer Tools: Linux, Git (GitHub, Gerrit), Anaconda, Jupyter, '\n",
      "           'Azure DevOps, Jenkins, Vim, Docker\\n'\n",
      "           'Databases: PostgreSQL, MySQL, Neo4J'}\n"
     ]
    }
   ],
   "source": [
    "pprint(parsed_resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
