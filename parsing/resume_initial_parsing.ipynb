{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Initial Parsing\n",
    "\n",
    "Before doing more fine-grained parsing on individual resume sections (Education, Experience, etc.), we need to parse the sections out\n",
    "\n",
    "Couple of options:\n",
    "\n",
    "1. Regex-based\n",
    "2. Text-only LLM\n",
    "3. Vision LLM (for PDFs that are not formatted with text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "from pydantic import BaseModel, RootModel, Field\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "\n",
    "from utils.with_structured_output import with_structured_output\n",
    "from utils.extract_pdf_text import extract_pdf_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Regex-based parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading_map = {\n",
    "    # Experience\n",
    "    r\"(Work|Relevant|Professional)?\\s*(Experience|History)\": \"Experience\",\n",
    "    r\"(Employment|Career)\\s*(History|Experience)\": \"Experience\",\n",
    "    r\"(Internship|Internships|Intern Experiences?)\": \"Experience\",\n",
    "    r\"(Freelance|Contract)\\s*(Work|Experience)\": \"Experience\",\n",
    "    r\"Work\": \"Experience\",\n",
    "\n",
    "    # Education\n",
    "    r\"(Education|Educational Background|Academic History|Academic Background)\": \"Education\",\n",
    "    r\"(Certifications|Courses|Licenses|Trainings|Accreditations)\": \"Certifications\",\n",
    "    r\"(Professional Development|Learning)\": \"Certifications\",\n",
    "\n",
    "    # Skills\n",
    "    r\"(Skills|Technical Skills|Key Competencies|Core Competencies|Abilities)\": \"Skills\",\n",
    "    r\"(Technical Proficiencies|Technical Expertise|Expertise|Proficiencies)\": \"Skills\",\n",
    "    r\"(Languages|Programming Languages)\": \"Skills\",\n",
    "    \n",
    "    # Projects\n",
    "    r\"(Projects|Key Projects|Personal Projects|Side Projects)\": \"Projects\",\n",
    "    r\"(Freelance Projects|Independent Projects|Portfolio)\": \"Projects\",\n",
    "\n",
    "    # Achievements and Awards\n",
    "    r\"(Achievements?|Awards?|Honors?|Accolades?|Recognitions?)\": \"Achievements\",\n",
    "    r\"(Accomplishments|Milestones)\": \"Achievements\",\n",
    "\n",
    "    # Volunteer Work\n",
    "    r\"(Volunteer|Volunteering|Community( Service)?|Volunteer Experience)\": \"Volunteer Work\",\n",
    "    r\"(Social Work|Non-Profit Work)\": \"Volunteer Work\",\n",
    "\n",
    "    # Leadership\n",
    "    r\"(Leadership|Leadership Experience|Leadership Roles|Positions of Responsibility)\": \"Leadership\",\n",
    "    r\"(Managerial Experience|Team Leadership|Organizational Roles)\": \"Leadership\",\n",
    "\n",
    "    # Publications and Research\n",
    "    r\"(Publications?|Research|Academic Papers|Articles|Journals?)\": \"Publications\",\n",
    "    r\"(Research Projects|Thesis|Dissertation)\": \"Research\",\n",
    "\n",
    "    # Interests and Hobbies\n",
    "    r\"(Interests?|Hobbies?|(Extracurricular|Collegiate) Activities)\": \"Interests\",\n",
    "    r\"(Passions?|Leisure Activities)\": \"Interests\",\n",
    "\n",
    "    # Objective or Summary\n",
    "    r\"(Objective|Career Objective|Professional Objective)\": \"Summary\",\n",
    "    r\"(Summary|Professional (Highlights|Profile|Summary)|Career Summary)\": \"Summary\",\n",
    "\n",
    "    # References\n",
    "    r\"(References?|Professional References|Referees?)\": \"References\",\n",
    "}\n",
    "\n",
    "def normalize_heading(heading):\n",
    "    for pattern, normalized_heading in heading_map.items():\n",
    "        if re.search(pattern, heading, re.IGNORECASE):\n",
    "            return normalized_heading\n",
    "    return \"Miscellaneous\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sections_by_heading(resume_text: str) -> dict[str, str]:\n",
    "    heading_pattern = r\"\"\"\n",
    "        ^(                                  \n",
    "            [A-Z][a-z]+(?:\\ [A-Z][a-z]+)?(?:[\\s]*\\n)    # Matches Captialized headings\n",
    "            |\n",
    "            [A-Z]{3,}(?:\\ [A-Z]{2,})*(?::?[\\s]*\\n)      # Matches ALL CAPS headings \n",
    "        )\n",
    "    \"\"\"\n",
    "    heading_regex = re.compile(heading_pattern, re.VERBOSE | re.MULTILINE)\n",
    "    matches = list(re.finditer(heading_regex, resume_text))\n",
    "    \n",
    "    sections = defaultdict(list)\n",
    "    for i, match in enumerate(matches):\n",
    "        # The section starts at the end of the heading\n",
    "        start = match.end()\n",
    "        # The section ends at the start of the next heading or the end of the resume\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(resume_text)\n",
    "        \n",
    "        heading = match.group(1).strip()\n",
    "        normalized_heading = normalize_heading(heading)\n",
    "        \n",
    "        if normalized_heading == \"Miscellaneous\":\n",
    "            sections[normalized_heading].append(resume_text[start:end].strip())\n",
    "        else :\n",
    "            sections[normalized_heading] = resume_text[start:end].strip()\n",
    "    \n",
    "    return dict(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Education': 'Texas A&M University, College Station, Texas December 2025\\n'\n",
      "              'Bachelor of Science in Aerospace Engineering, Minor in '\n",
      "              'Mathematics\\n'\n",
      "              'GPA: 4.0',\n",
      " 'Experience': 'Sandia National Laboratories, Albuquerque, New Mexico\\n'\n",
      "               'Applied Aerodynamics R&D Intern Aug 2024 – Present\\n'\n",
      "               'Nuclear Deterrence R&D Intern May 2024 – Aug 2024\\n'\n",
      "               'NASA Armstrong Flight Research Center, Edwards, California Jan '\n",
      "               '2024 – May 2024\\n'\n",
      "               'Aerostructures Research Engineering Intern\\n'\n",
      "               '• Conducted disk (bubble) testing of nylon fabric to validate '\n",
      "               'strain sensors for use on parachute canopies.\\n'\n",
      "               '• Directed 12 disk tests to failure to investigate fabric '\n",
      "               'failure modes and maximum operating pressure.\\n'\n",
      "               '• Programmed MATLAB script to measure strains up to 20% from '\n",
      "               'side-profile images of disk tests.\\n'\n",
      "               '• Directed 10 tests to validate side-profile script against '\n",
      "               'strain measured by Digital Image Correlation (DIC).\\n'\n",
      "               '• Analyzed DIC data using GOM software, yielding script '\n",
      "               'relative error below 5% on average.\\n'\n",
      "               '• Directed 10 disk tests of nylon embedded with a 2-inch '\n",
      "               'commercial off-the-shelf capacitive strain sensor.\\n'\n",
      "               '• Used side-profile script to measure strain under the '\n",
      "               'capacitive sensor as a source of validation data.\\n'\n",
      "               '• Laser-cut over 100 nylon and silicon coupons for disk '\n",
      "               'testing of parachute broadcloth.\\n'\n",
      "               '• Added configuration-saving functionality to Python GUI '\n",
      "               'developed for a NASA-made portable DAQ.\\n'\n",
      "               'Klebanoff-Saric Wind Tunnel, College Station, Texas Jun 2023 – '\n",
      "               'Dec 2023\\n'\n",
      "               'Undergraduate Research Assistant\\n'\n",
      "               '• Developed LabVIEW code to operate tunnel fan and National '\n",
      "               'Instruments DAQ during daily calibration.\\n'\n",
      "               '• Created Python scripts to compute 7 nonlinear regression '\n",
      "               'parameters relating wind speed to Constant\\n'\n",
      "               'Temperature Anemometer voltage and tunnel temperature, and '\n",
      "               'integrated these scripts into LabVIEW code.\\n'\n",
      "               '• Programmed Monte Carlo simulation (500+ realizations) to '\n",
      "               'estimate parameter uncertainty from sensor error.\\n'\n",
      "               '• Wrote 10-page report detailing integration of Python and '\n",
      "               'LabVIEW scripts.\\n'\n",
      "               '• Increased data visualization efficiency by writing Python '\n",
      "               'script to load and format data in Tecplot360.\\n'\n",
      "               '• Operated a low-speed (2-15 m/s) wind tunnel in support of '\n",
      "               'boundary layer experiments involving Constant\\n'\n",
      "               'Temperature Anemometry and naphthalene flow visualization.\\n'\n",
      "               'Technical Projects and Research\\n'\n",
      "               'Texas A&M AIAA CanSat Team, College Station, Texas Oct 2022 – '\n",
      "               'May 2024\\n'\n",
      "               'Electrical Team Lead\\n'\n",
      "               '• Designed electrical power and avionics subsystems of a 300 '\n",
      "               'cubic inch satellite for AIAA’s annual CanSat\\n'\n",
      "               'design-build-launch competition with a team of two other '\n",
      "               'students.\\n'\n",
      "               '• Created schedules for electrical system progress and '\n",
      "               'assigned tasks to team members.\\n'\n",
      "               '• Utilized EasyEDA to create preliminary PCB design, fitting '\n",
      "               'over 15 components into a 63 mm radius space.\\n'\n",
      "               '• Performed trade studies of air pressure sensors, pitot '\n",
      "               'tubes, microcontrollers, GPS units, XBee radios,\\n'\n",
      "               'gyroscopes, and cameras with a $1000 budget and 700g mass '\n",
      "               'budget.\\n'\n",
      "               '• Presented 10 slides covering the electrical power subsystem, '\n",
      "               'microcontroller, and inertial measurement unit\\n'\n",
      "               'to competition judges as part of the Preliminary Design Review '\n",
      "               'and Cumulative Design Review.\\n'\n",
      "               '• Soldered over 20 electrical components and 2 independent '\n",
      "               'electrical systems on satellite, including avionics\\n'\n",
      "               'and power for each system.\\n'\n",
      "               'Texas A&M Turbomachinery Lab, College Station, Texas Feb 2023 '\n",
      "               '– Dec 2023',\n",
      " 'Miscellaneous': ['www.linkedin.com/in/bmayeux03 ♦ bmayeux@tamu.edu'],\n",
      " 'Publications': '• Conducted research aiming to predict hazards associated '\n",
      "                 'with Li-Ion battery combustion, including amount of\\n'\n",
      "                 'energy released, final temperature, and chemical composition '\n",
      "                 'of reaction products.\\n'\n",
      "                 '• Reviewed scientific literature to determine thermal '\n",
      "                 'decomposition properties of various battery cathodes.'}\n"
     ]
    }
   ],
   "source": [
    "resume_text = extract_pdf_text(\"../sample-data/Ben_Resume.pdf\")\n",
    "resume_sections = extract_sections_by_heading(resume_text)\n",
    "pprint(resume_sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex-based parsing is too inflexible for handling a wide variety of resume formats... maybe an LLM-based solution would work better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: LLM with Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resume(BaseModel):\n",
    "    experience: str = Field(..., alias=\"Experience\")\n",
    "    education: str  = Field(..., alias=\"Education\")\n",
    "    skills: str     = Field(..., alias=\"Skills\")\n",
    "    projects: str   = Field(..., alias=\"Projects\")\n",
    "    leadership: str = Field(..., alias=\"Leadership\")\n",
    "    research: str   = Field(..., alias=\"Research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_EXTRACTION_PROMPT = \"\"\"\n",
    "You are an expert at parsing resume information. Given resume text, your job is to parse individual sections based on resume heading. Follow this format:\n",
    "    {{\n",
    "        \"Experience\": \"<Experience>\",\n",
    "        \"Education\": \"<Education>\",\n",
    "        \"Skills\": \"<Skills>\",\n",
    "        \"Projects\": \"<Projects>\",\n",
    "        \"Leadership\": \"<Leadership Experience>\",\n",
    "        \"Research\": \"<Research Experience>\"\n",
    "    }}\n",
    "    \n",
    "Your job is very simple: simply copy everything under each resume section into the output format; do not worry about formatting. \n",
    "\n",
    "If a resume does not contain one of the sections, output an empty string for that section. For example, if there is no \"Leadership\" section in the resume, the output will be `\"Leadership\": \"\"`.\n",
    "\n",
    "In the experience section, **ensure that you include company names**, which are usually listed beside or under position names.\n",
    "\n",
    "**The parsed information must be explicitly contained in the resume.**\n",
    "\n",
    "**Do not exclude any information from the resume.**\n",
    "\n",
    "Resume:\n",
    "{resume_text}\n",
    "\n",
    "Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_text = extract_pdf_text(\"../sample-data/Raquel-resume.pdf\")\n",
    "parsed_resume = with_structured_output(\n",
    "    prompt=INITIAL_EXTRACTION_PROMPT.format(resume_text=resume_text),\n",
    "    schema=Resume,\n",
    "    model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/parsed_resume.json\", \"w\") as file:\n",
    "    json.dump(parsed_resume, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nFinal Answer: The final answer is 100. I hope it is correct. \\n\\nPlease let me know if I need to make any changes. \\n\\nI will make sure to follow the format to the letter as requested. \\n\\nThe final answer is'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_API_KEY = os.getenv(\"HF_API_KEY\")\n",
    "client = InferenceClient(token=HF_API_KEY)\n",
    "\n",
    "response = client.text_generation(\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    prompt=\"Answer concisely: What is 10 x 10?\",\n",
    "    max_new_tokens=50)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "(Request ID: 6pyDhKkwQmqlIJUXhutQk)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/resume/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/resume/lib/python3.12/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.1-8B-Instruct",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-3.1-8B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINITIAL_EXTRACTION_PROMPT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_text\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mResume\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_json_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/resume/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:2332\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[0;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[1;32m   2307\u001b[0m         _set_unsupported_text_generation_kwargs(model, unused_params)\n\u001b[1;32m   2308\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_generation(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2309\u001b[0m             prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m   2310\u001b[0m             details\u001b[38;5;241m=\u001b[39mdetails,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2330\u001b[0m             watermark\u001b[38;5;241m=\u001b[39mwatermark,\n\u001b[1;32m   2331\u001b[0m         )\n\u001b[0;32m-> 2332\u001b[0m     \u001b[43mraise_text_generation_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[38;5;66;03m# Parse output\u001b[39;00m\n\u001b[1;32m   2335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/resume/lib/python3.12/site-packages/huggingface_hub/inference/_common.py:466\u001b[0m, in \u001b[0;36mraise_text_generation_error\u001b[0;34m(http_error)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhttp_error\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# Otherwise, fallback to default error\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m http_error\n",
      "File \u001b[0;32m/opt/anaconda3/envs/resume/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:2302\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[0;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[1;32m   2300\u001b[0m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[1;32m   2301\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2302\u001b[0m     bytes_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2303\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2304\u001b[0m     match \u001b[38;5;241m=\u001b[39m MODEL_KWARGS_NOT_USED_REGEX\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/resume/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:296\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 296\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/resume/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:460\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[1;32m    457\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m endpoint:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m endpoint_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m     )\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(BadRequestError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m    463\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure your token has the correct permissions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    467\u001b[0m     )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: (Request ID: 6pyDhKkwQmqlIJUXhutQk)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query."
     ]
    }
   ],
   "source": [
    "response = client.text_generation(\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    prompt=INITIAL_EXTRACTION_PROMPT.format(resume_text=resume_text),\n",
    "    grammar={\n",
    "        \"type\": \"json\",\n",
    "        \"value\": Resume.model_json_schema()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need HF Pro subscription to access some Llama models; I think Ollama might be my best option after all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3: Vision Model Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_pdf_text(\"../sample_data/Kevin_resume_img.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the PDF to an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 1\n"
     ]
    }
   ],
   "source": [
    "images = convert_from_path(\"../sample_data/Ben_resume.pdf\", dpi=300)\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    image.save(f\"output/Ben_resume.jpg\", \"JPEG\")\n",
    "    print(f\"Saved page {i + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the vision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_EXTRACTION_PROMPT = \"\"\"\n",
    "You are an expert at parsing resume information from an image. Given an image of a resume, your job is to parse individual sections based on resume heading. Follow this format:\n",
    "    {{\n",
    "        \"Experience\": \"<Experience>\",\n",
    "        \"Education\": \"<Education>\",\n",
    "        \"Skills\": \"<Skills>\",\n",
    "        \"Projects\": \"<Projects>\",\n",
    "        \"Leadership\": \"<Leadership Experience>\",\n",
    "        \"Research\": \"<Research Experience>\"\n",
    "    }}\n",
    "    \n",
    "Your job is very simple: simply copy everything under each resume section into the output format; do not worry about formatting. \n",
    "\n",
    "If a resume does not contain one of the sections, output an empty string for that section. For example, if there is no \"Leadership\" section in the resume, the output will be `\"Leadership\": \"\"`.\n",
    "\n",
    "In the experience section, **ensure that you include company names**, which are usually listed beside or under position names.\n",
    "\n",
    "**The parsed information must be explicitly contained in the resume.**\n",
    "\n",
    "**Do not exclude any information from the resume.**\n",
    "\n",
    "Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "\n",
    "response = chat(\n",
    "    model=\"llama3.2-vision\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": IMAGE_EXTRACTION_PROMPT,\n",
    "            \"images\": [\"output/Ben_resume.jpg\"]\n",
    "        }\n",
    "    ],\n",
    "    format=Resume.model_json_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Education': 'Texas A&M University, College Station, Texas: Bachelor of '\n",
      "              'Science in Aerospace Engineering, Minor in Mathematics, GPA 4.0',\n",
      " 'Experience': 'Sandia National Laboratories, Albuquerque, New Mexico: Applied '\n",
      "               'Aerodynamics R&D Intern; NASA Armstrong Flight Research '\n",
      "               'Center, Edwards, California: Aerostructures Research '\n",
      "               'Engineering Intern; Klebanoff-Saric Wind Tunnel, College '\n",
      "               'Station, Texas: Undergraduate Research Assistant',\n",
      " 'Leadership': '',\n",
      " 'Projects': 'Klebanoff-Saric Wind Tunnel, College Station, Texas: Developed '\n",
      "             'LabVIEW code to operate tunnel fan and National Instruments DAQ '\n",
      "             'during daily calibration.',\n",
      " 'Research': 'Texas A&M University, College Station, Texas: Undergraduate '\n",
      "             'Researcher',\n",
      " 'Skills': ''}\n"
     ]
    }
   ],
   "source": [
    "parsed_resume = json.loads(response.message.content)\n",
    "pprint(parsed_resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bad with VLMs:\n",
    "1. Most state-of-the-art VLMs cannot process very large images (e.g., Llama3.2-vision can only process images up to 1120x1120)\n",
    "2. llama3.2-vision runs pretty slow on my computer, probably because it requires 11B parameters vs only 8B from llama3.1\n",
    "3. llama3.2-vision isn't very good at parsing a lot of textual information from resumes; it's missing a lot of information from the \"Experience\" section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Option 4: OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_images = convert_from_path(\"../sample_data/Kevin_resume.pdf\", dpi=300)\n",
    "pdf_images[0].save(\"output/Kevin_resume.png\", \"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
